---
title: "Exploratory Data Analysis"
author: "Riccardo Finotello"
date: "16/07/2020"
output:
    html_document:
        keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Getting the Data

As a first step in the analysis we first download and unzip the [dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) used in the analysis.
```{r download}
file.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
file.out <- "../dataset.zip"

if(!file.exists(file.out)) {download.file(file.url, file.out, method="curl")}
if(!dir.exists("../final")) {unzip(file.out)}
```

We also download a list of profanities and bad words we exclude from the analysis.
The list can be found on Google's [Code Archive](https://code.google.com/archive/p/badwordslist/downloads).
```{r download.badwords}
file.url <- "https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/badwordslist/badwords.txt"
file.out <- "../badwords.txt"

if(!file.exists(file.out)) {download.file(file.url, file.out, method="curl")}
```

We then subsample the files for training, validation and test (we use the English versions of the files).
Each file is around 200 MB in size with at least $9 \times 10^5$ (up to more than $2 \times 10^6$) lines: we will use a limited amount of samples for training (to speed it up).
In fact we can use around 50% of the data for training and then subsample one more time 5% for validation and 5% for testing purposes what remains: this way we will have enough data for proper training and a significant sample for evaluation.
```{r sample, cache=TRUE}
# read the files
blogs   <- scan(file="../final/en_US/en_US.blogs.txt",
                blank.lines.skip=TRUE,
                what="character",
                fileEncoding="UTF-8"
               )
news    <- scan(file="../final/en_US/en_US.news.txt",
                blank.lines.skip=TRUE,
                what="character",
                fileEncoding="UTF-8"
               )
twitter <- scan(file="../final/en_US/en_US.twitter.txt",
                blank.lines.skip=TRUE,
                what="character",
                fileEncoding="UTF-8"
               )

# clean from non ASCII characters
blogs   <- iconv(blogs, to="ASCII", sub="")
news    <- iconv(news, to="ASCII", sub="")
twitter <- iconv(twitter, to="ASCII", sub="")

# sample training data (flip a biased coin and decide what to keep)
set.seed(42)

# blogs data
index <- as.logical(rbinom(n=length(blogs), size=1, prob=0.5))
blogs.train <- blogs[index]
blogs.out   <- blogs[-index]

index <- as.logical(rbinom(n=length(blogs.out), size=1, prob=0.05))
blogs.val   <- blogs.out[index]
blogs.out   <- blogs.out[-index]

index <- as.logical(rbinom(n=length(blogs.out), size=1, prob=0.05))
blogs.test  <- blogs.out[index]

# news data
index <- as.logical(rbinom(n=length(news), size=1, prob=0.5))
news.train <- news[index]
news.out   <- news[-index]

index <- as.logical(rbinom(n=length(news.out), size=1, prob=0.05))
news.val   <- news.out[index]
news.out   <- news.out[-index]

index <- as.logical(rbinom(n=length(news.out), size=1, prob=0.05))
news.test  <- news.out[index]

# twitter data
index <- as.logical(rbinom(n=length(twitter), size=1, prob=0.5))
twitter.train <- twitter[index]
twitter.out   <- twitter[-index]

index <- as.logical(rbinom(n=length(twitter.out), size=1, prob=0.05))
twitter.val   <- twitter.out[index]
twitter.out   <- twitter.out[-index]

index <- as.logical(rbinom(n=length(twitter.out), size=1, prob=0.05))
twitter.test  <- twitter.out[index]
```

We finally save training, validation and test data to separate files..
```{r cleanup}
# create directories
if(!dir.exists("../train")) {dir.create("../train")}
if(!dir.exists("../val")) {dir.create("../val")}
if(!dir.exists("../test")) {dir.create("../test")}

# save files
writeLines(blogs.train, "../train/blogs.txt")
writeLines(blogs.val, "../val/blogs.txt")
writeLines(blogs.test, "../test/blogs.txt")
writeLines(news.train, "../train/news.txt")
writeLines(news.val, "../val/news.txt")
writeLines(news.test, "../test/news.txt")
writeLines(twitter.train, "../train/twitter.txt")
writeLines(twitter.val, "../val/twitter.txt")
writeLines(twitter.test, "../test/twitter.txt")
```

```{r cleanup, echo=FALSE, include=FALSE}
rm(list=ls())
gc()
```

## Tokenization and Profanity Filtering

We then start processing the data.
We extract the tokens from the training, validation and test sets and remove profanities from the training set: we do not want the algorithm to learn bad words, but real word application have them.
```{r token, cache=TRUE}
library(quanteda)
library(readtext)

# set multithreading
quanteda_options("threads"=6)

# load corpus
corpus.train <- corpus(readtext("../train/*.txt"))

# load badwords
badwords <- scan(file="../badwords.txt",
                 blank.lines.skip=TRUE,
                 what="character",
                 fileEncoding="UTF-8"
                )
badwords <- tolower(badwords)
```

We now have two separate objects: one corpus for the training words and a character vector for the bad words.
The next step is tokenization.
We use `quanteda`'s function `tokens` to separate tokens, remove punctuation and symbols, and split hyphen-separated words.
We also directly remove the profanities before saving everything to file.
```{r token, cache=TRUE}
# tokenize the training data (remove sentences shorter than 3 tokens)
corpus.train <- corpus_trim(corpus.train,
                            what="sentences",
                            min_ntoken=4
                           )
tokens.train <- tokens(tolower(corpus.train),
                       remove_punct=TRUE,
                       remove_url=TRUE
                      )
tokes.train <- tokens_select(tokens.train,
                             pattern=badwords,
                             selection="remove",
                             case_insensitive=TRUE
                            )
save(tokens.train, file="../train/tokens.train.Rdata")
```